{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'docs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptax\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mox\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocs\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexamples\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clean_legend\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m install_import_hook(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpjax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbeartype.beartype\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgpjax\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgpx\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'docs'"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# jupyter:\n",
    "#   jupytext:\n",
    "#     cell_metadata_filter: -all\n",
    "#     custom_cell_magics: kql\n",
    "#     text_representation:\n",
    "#       extension: .py\n",
    "#       format_name: percent\n",
    "#       format_version: '1.3'\n",
    "#       jupytext_version: 1.11.2\n",
    "#   kernelspec:\n",
    "#     display_name: gpjax\n",
    "#     language: python\n",
    "#     name: python3\n",
    "# ---\n",
    "\n",
    "# %% [markdown]\n",
    "# # Regression\n",
    "#\n",
    "# In this notebook we demonstate how to fit a Gaussian process regression model.\n",
    "\n",
    "# %%\n",
    "# Enable Float64 for more stable matrix inversions.\n",
    "from jax import config\n",
    "\n",
    "config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "from jaxtyping import install_import_hook\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import optax as ox\n",
    "# from docs.examples.utils import clean_legend\n",
    "\n",
    "with install_import_hook(\"gpjax\", \"beartype.beartype\"):\n",
    "    import gpjax as gpx\n",
    "\n",
    "key = jr.PRNGKey(123)\n",
    "plt.style.use(\n",
    "    \"https://raw.githubusercontent.com/JaxGaussianProcesses/GPJax/main/docs/examples/gpjax.mplstyle\"\n",
    ")\n",
    "cols = mpl.rcParams[\"axes.prop_cycle\"].by_key()[\"color\"]\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Dataset\n",
    "#\n",
    "# With the necessary modules imported, we simulate a dataset\n",
    "# $\\mathcal{D} = (\\boldsymbol{x}, \\boldsymbol{y}) = \\{(x_i, y_i)\\}_{i=1}^{100}$ with inputs $\\boldsymbol{x}$\n",
    "# sampled uniformly on $(-3., 3)$ and corresponding independent noisy outputs\n",
    "#\n",
    "# $$\\boldsymbol{y} \\sim \\mathcal{N} \\left(\\sin(4\\boldsymbol{x}) + \\cos(2 \\boldsymbol{x}), \\textbf{I} * 0.3^2 \\right).$$\n",
    "#\n",
    "# We store our data $\\mathcal{D}$ as a GPJax `Dataset` and create test inputs and labels\n",
    "# for later.\n",
    "\n",
    "# %%\n",
    "n = 100\n",
    "noise = 0.3\n",
    "\n",
    "key, subkey = jr.split(key)\n",
    "x = jr.uniform(key=key, minval=-3.0, maxval=3.0, shape=(n,)).reshape(-1, 1)\n",
    "f = lambda x: jnp.sin(4 * x) + jnp.cos(2 * x)\n",
    "signal = f(x)\n",
    "y = signal + jr.normal(subkey, shape=signal.shape) * noise\n",
    "\n",
    "D = gpx.Dataset(X=x, y=y)\n",
    "\n",
    "xtest = jnp.linspace(-3.5, 3.5, 500).reshape(-1, 1)\n",
    "ytest = f(xtest)\n",
    "\n",
    "# %% [markdown]\n",
    "# To better understand what we have simulated, we plot both the underlying latent\n",
    "# function and the observed data that is subject to Gaussian noise.\n",
    "\n",
    "# %%\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y, \"o\", label=\"Observations\", color=cols[0])\n",
    "ax.plot(xtest, ytest, label=\"Latent function\", color=cols[1])\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# %% [markdown]\n",
    "# Our aim in this tutorial will be to reconstruct the latent function from our noisy\n",
    "# observations $\\mathcal{D}$ via Gaussian process regression. We begin by defining a\n",
    "# Gaussian process prior in the next section.\n",
    "#\n",
    "# ## Defining the prior\n",
    "#\n",
    "# A zero-mean Gaussian process (GP) places a prior distribution over real-valued\n",
    "# functions $f(\\cdot)$ where\n",
    "# $f(\\boldsymbol{x}) \\sim \\mathcal{N}(0, \\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}})$\n",
    "# for any finite collection of inputs $\\boldsymbol{x}$.\n",
    "#\n",
    "# Here $\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}$ is the Gram matrix generated by a\n",
    "# user-specified symmetric, non-negative definite kernel function $k(\\cdot, \\cdot')$\n",
    "# with $[\\mathbf{K}_{\\boldsymbol{x}\\boldsymbol{x}}]_{i, j} = k(x_i, x_j)$.\n",
    "# The choice of kernel function is critical as, among other things, it governs the\n",
    "# smoothness of the outputs that our GP can generate.\n",
    "#\n",
    "# For simplicity, we consider a radial basis function (RBF) kernel:\n",
    "# $$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\lVert x - x' \\rVert_2^2}{2 \\ell^2}\\right).$$\n",
    "#\n",
    "# On paper a GP is written as $f(\\cdot) \\sim \\mathcal{GP}(\\textbf{0}, k(\\cdot, \\cdot'))$,\n",
    "# we can reciprocate this process in GPJax via defining a `Prior` with our chosen `RBF`\n",
    "# kernel.\n",
    "\n",
    "# %%\n",
    "kernel = gpx.kernels.RBF()\n",
    "meanf = gpx.mean_functions.Zero()\n",
    "prior = gpx.gps.Prior(mean_function=meanf, kernel=kernel)\n",
    "\n",
    "# %% [markdown]\n",
    "#\n",
    "# The above construction forms the foundation for GPJax's models. Moreover, the GP prior\n",
    "# we have just defined can be represented by a\n",
    "# [TensorFlow Probability](https://www.tensorflow.org/probability/api_docs/python/tfp/substrates/jax)\n",
    "# multivariate Gaussian distribution. Such functionality enables trivial sampling, and\n",
    "# the evaluation of the GP's mean and covariance .\n",
    "\n",
    "# %%\n",
    "prior_dist = prior.predict(xtest)\n",
    "\n",
    "prior_mean = prior_dist.mean()\n",
    "prior_std = prior_dist.variance()\n",
    "samples = prior_dist.sample(seed=key, sample_shape=(20,))\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xtest, samples.T, alpha=0.5, color=cols[0], label=\"Prior samples\")\n",
    "ax.plot(xtest, prior_mean, color=cols[1], label=\"Prior mean\")\n",
    "ax.fill_between(\n",
    "    xtest.flatten(),\n",
    "    prior_mean - prior_std,\n",
    "    prior_mean + prior_std,\n",
    "    alpha=0.3,\n",
    "    color=cols[1],\n",
    "    label=\"Prior variance\",\n",
    ")\n",
    "ax.legend(loc=\"best\")\n",
    "# ax = clean_legend(ax)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Constructing the posterior\n",
    "#\n",
    "# Having defined our GP, we proceed to define a description of our data\n",
    "# $\\mathcal{D}$ conditional on our knowledge of $f(\\cdot)$ --- this is exactly the\n",
    "# notion of a likelihood function $p(\\mathcal{D} | f(\\cdot))$. While the choice of\n",
    "# likelihood is a critical in Bayesian modelling, for simplicity we consider a\n",
    "# Gaussian with noise parameter $\\alpha$\n",
    "# $$p(\\mathcal{D} | f(\\cdot)) = \\mathcal{N}(\\boldsymbol{y}; f(\\boldsymbol{x}), \\textbf{I} \\alpha^2).$$\n",
    "# This is defined in GPJax through calling a `Gaussian` instance.\n",
    "\n",
    "# %%\n",
    "likelihood = gpx.likelihoods.Gaussian(num_datapoints=D.n)\n",
    "\n",
    "# %% [markdown]\n",
    "# The posterior is proportional to the prior multiplied by the likelihood, written as\n",
    "#\n",
    "#   $$ p(f(\\cdot) | \\mathcal{D}) \\propto p(f(\\cdot)) * p(\\mathcal{D} | f(\\cdot)). $$\n",
    "#\n",
    "# Mimicking this construct, the posterior is established in GPJax through the `*` operator.\n",
    "\n",
    "# %%\n",
    "posterior = prior * likelihood\n",
    "\n",
    "# %% [markdown]\n",
    "# <!-- ## Hyperparameter optimisation\n",
    "#\n",
    "# Our kernel is parameterised by a length-scale $\\ell^2$ and variance parameter\n",
    "# $\\sigma^2$, while our likelihood controls the observation noise with $\\alpha^2$.\n",
    "# Using Jax's automatic differentiation module, we can take derivatives of  -->\n",
    "#\n",
    "# ## Parameter state\n",
    "#\n",
    "# As outlined in the [PyTrees](https://jax.readthedocs.io/en/latest/pytrees.html)\n",
    "# documentation, parameters are contained within the model and for the leaves of the\n",
    "# PyTree. Consequently, in this particular model, we have three parameters: the\n",
    "# kernel lengthscale, kernel variance and the observation noise variance. Whilst\n",
    "# we have initialised each of these to 1, we can learn Type 2 MLEs for each of\n",
    "# these parameters by optimising the marginal log-likelihood (MLL).\n",
    "\n",
    "# %%\n",
    "negative_mll = gpx.objectives.ConjugateMLL(negative=True)\n",
    "negative_mll(posterior, train_data=D)\n",
    "\n",
    "\n",
    "# static_tree = jax.tree_map(lambda x: not(x), posterior.trainables)\n",
    "# optim = ox.chain(\n",
    "#     ox.adam(learning_rate=0.01),\n",
    "#     ox.masked(ox.set_to_zero(), static_tree)\n",
    "#     )\n",
    "# %% [markdown]\n",
    "# For researchers, GPJax has the capacity to print the bibtex citation for objects such\n",
    "# as the marginal log-likelihood through the `cite()` function.\n",
    "\n",
    "# %%\n",
    "print(gpx.cite(negative_mll))\n",
    "\n",
    "# %% [markdown]\n",
    "# JIT-compiling expensive-to-compute functions such as the marginal log-likelihood is\n",
    "# advisable. This can be achieved by wrapping the function in `jax.jit()`.\n",
    "\n",
    "# %%\n",
    "negative_mll = jit(negative_mll)\n",
    "\n",
    "# %% [markdown]\n",
    "# Since most optimisers (including here) minimise a given function, we have realised\n",
    "# the negative marginal log-likelihood and just-in-time (JIT) compiled this to\n",
    "# accelerate training.\n",
    "\n",
    "# %% [markdown]\n",
    "# We can now define an optimiser. For this example we'll use the `bfgs`\n",
    "# optimiser.\n",
    "\n",
    "# %%\n",
    "opt_posterior, history = gpx.fit_scipy(\n",
    "    model=posterior,\n",
    "    objective=negative_mll,\n",
    "    train_data=D,\n",
    ")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Prediction\n",
    "#\n",
    "# Equipped with the posterior and a set of optimised hyperparameter values, we are now\n",
    "# in a position to query our GP's predictive distribution at novel test inputs. To do\n",
    "# this, we use our defined `posterior` and `likelihood` at our test inputs to obtain\n",
    "# the predictive distribution as a `Distrax` multivariate Gaussian upon which `mean`\n",
    "# and `stddev` can be used to extract the predictive mean and standard deviatation.\n",
    "\n",
    "# %%\n",
    "latent_dist = opt_posterior.predict(xtest, train_data=D)\n",
    "predictive_dist = opt_posterior.likelihood(latent_dist)\n",
    "\n",
    "predictive_mean = predictive_dist.mean()\n",
    "predictive_std = predictive_dist.stddev()\n",
    "\n",
    "# %% [markdown]\n",
    "# With the predictions and their uncertainty acquired, we illustrate the GP's\n",
    "# performance at explaining the data $\\mathcal{D}$ and recovering the underlying\n",
    "# latent function of interest.\n",
    "\n",
    "# %%\n",
    "fig, ax = plt.subplots(figsize=(7.5, 2.5))\n",
    "ax.plot(x, y, \"x\", label=\"Observations\", color=cols[0], alpha=0.5)\n",
    "ax.fill_between(\n",
    "    xtest.squeeze(),\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    alpha=0.2,\n",
    "    label=\"Two sigma\",\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean - 2 * predictive_std,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest,\n",
    "    predictive_mean + 2 * predictive_std,\n",
    "    linestyle=\"--\",\n",
    "    linewidth=1,\n",
    "    color=cols[1],\n",
    ")\n",
    "ax.plot(\n",
    "    xtest, ytest, label=\"Latent function\", color=cols[0], linestyle=\"--\", linewidth=2\n",
    ")\n",
    "ax.plot(xtest, predictive_mean, label=\"Predictive mean\", color=cols[1])\n",
    "ax.legend(loc=\"center left\", bbox_to_anchor=(0.975, 0.5))\n",
    "\n",
    "# %% [markdown]\n",
    "# ## System configuration\n",
    "\n",
    "# %%\n",
    "# %reload_ext watermark\n",
    "# %watermark -n -u -v -iv -w -a 'Thomas Pinder & Daniel Dodd'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GaussianProcessProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
