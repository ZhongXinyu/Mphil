{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a7ad90f-b09b-4529-8194-30fb2bc45ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05899553-928d-4c8c-820b-dc1a61050ea4",
   "metadata": {},
   "source": [
    "For simplicity, we will demonstrate nested sampling in a 2-dimension unit square, $\\mathbf{x}=(x_0, x_1)$, with the uniform prior \n",
    "$$ \\pi(\\mathbf{x}) = \\begin{cases} 1 \\quad \\mathrm{if} \\; 0\\leq x_0, x_1 \\leq 1\\\\ 0 \\quad \\mathrm{else} \\end{cases} . $$\n",
    "\n",
    "We will explore the following simple likelihood function with exponent $\\epsilon=2$,\n",
    "$$ \\mathcal{L}(d|\\mathbf{x}) = ((x_0 x_1 (1-x_0) (1-x_1))^{\\epsilon} . $$  \n",
    "\n",
    "For this toy problem, the evidence can be calculated analytically; the result is expressed in terms of the beta function,\n",
    "$$ Z = \\int_0^1\\mathrm{d}x_0\\;\\int_0^1\\mathrm{d}x_1\\; \\mathcal{L}(d|\\mathbf{x}) \\pi(\\mathbf{x}) = B(\\epsilon+1, \\epsilon+1)^2 . $$\n",
    "We will use this as a check on our nested sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8c832f29-894c-4669-92e3-a009935a78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class nested_sampling_demo():\n",
    "\n",
    "    def __init__(self, exponent=2., num_live=10):\n",
    "        \"\"\" initialise nested sampling demo \"\"\"\n",
    "        self.dim = 2\n",
    "        self.exponent = exponent\n",
    "        self.num_live = num_live\n",
    "\n",
    "        # the exact, analytic evidence (for testing)\n",
    "        self.Zexact = beta(self.exponent+1, self.exponent+1)**2\n",
    "\n",
    "        self.centre_point = 0.5 * np.ones(self.dim)\n",
    "        self.peak_logL = self.log_likelihood(self.centre_point)\n",
    "\n",
    "        self.live_points = [ self.prior_sample() for j in range(self.num_live) ]\n",
    "        self.live_loglikelihoods = [ self.log_likelihood(x) for x in self.live_points ]\n",
    "\n",
    "        self.xis = [1.]\n",
    "        self.samples = []\n",
    "        self.weights = []\n",
    "        self.logLs = [-np.inf]\n",
    "        self.dead_point = None\n",
    "        \n",
    "        self.iteration = 0\n",
    "\n",
    "    def log_likelihood(self, x):\n",
    "        \"\"\" the log-likelihood function $\\log\\mathcal{L}(d|x)$ \"\"\"\n",
    "        return self.exponent * ( np.sum(np.log(x)) + np.sum(np.log(1-x)) ) \n",
    "\n",
    "    def prior_sample(self, logL=-np.inf):\n",
    "        \"\"\" sample the prior with constraint likelihood > L \"\"\"\n",
    "        assert logL<self.peak_logL, \"Warning: logL is too large\"\n",
    "        accept = False\n",
    "        while not accept:\n",
    "            x = np.random.uniform(size=2)\n",
    "            accept = (self.log_likelihood(x)>logL)\n",
    "        return x\n",
    "\n",
    "    def _iterate(self):\n",
    "        \"\"\" perform one iteration of nested sampling algorithm \"\"\"\n",
    "        self.iteration += 1\n",
    "\n",
    "        idx_min = np.argmin(self.live_loglikelihoods)\n",
    "\n",
    "        self.samples.append(self.live_points[idx_min])\n",
    "        \n",
    "        self.xis.append(np.exp(-self.iteration/self.num_live))\n",
    "        \n",
    "        self.weights.append( np.exp(-(self.iteration-1)/self.num_live) - np.exp(-(self.iteration)/self.num_live) )\n",
    "        \n",
    "        self.logLs.append(self.live_loglikelihoods[idx_min])\n",
    "        \n",
    "        self.dead_point = self.live_points[idx_min]\n",
    "        \n",
    "        self.replacement = self.prior_sample(logL=self.live_loglikelihoods[idx_min])\n",
    "        \n",
    "        self.live_points[idx_min] = self.replacement\n",
    "        \n",
    "        self.live_loglikelihoods[idx_min] = self.log_likelihood(self.live_points[idx_min])\n",
    "\n",
    "    def iterate(self, num_iter=1):\n",
    "        \"\"\" iterate and plot \"\"\"\n",
    "        for i in range(num_iter):\n",
    "            self._iterate()\n",
    "        self.plot()\n",
    "\n",
    "    def plot(self):\n",
    "        \"\"\" make plots showing current state of the algorithm \"\"\"\n",
    "        fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(7, 15))\n",
    "\n",
    "        # \n",
    "        border = 1.0e-3\n",
    "        x = np.linspace(border, 1-border, 50)\n",
    "        y = np.linspace(border, 1-border, 50)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        Z = np.zeros_like(X)\n",
    "        for i, x_ in enumerate(x):\n",
    "            for j, y_ in enumerate(y):\n",
    "                Z[i,j] = np.exp(self.log_likelihood(np.array([x_,y_])))\n",
    "        axs[0].contourf(X, Y, Z, 16, alpha=0.1)\n",
    "        axs[0].contour(X, Y, Z, [np.exp(self.log_likelihood(self.dead_point))], colors='C1', linewidths=1)\n",
    "        \n",
    "        axs[0].set_title(r'$N_{\\rm live}='+str(self.num_live)+'$ Points In Sample Space')\n",
    "        pts = np.array(self.live_points)\n",
    "        axs[0].scatter(pts[:,0], pts[:,1], color='C0')\n",
    "        axs[0].scatter(self.dead_point[0], self.dead_point[1], s=70, label='worst point ($\\log\\mathcal{L}='+\n",
    "                       '{:.1e}'.format(self.log_likelihood(self.dead_point))+'$)', color='C1')\n",
    "        axs[0].scatter(self.replacement[0], self.replacement[1], s=70, label='replacement ($\\log\\mathcal{L}='+\n",
    "                       '{:.1e}'.format(self.log_likelihood(self.replacement))+'$)', color='C2')\n",
    "        axs[0].arrow(self.dead_point[0], self.dead_point[1],\n",
    "                     self.replacement[0]-self.dead_point[0], self.replacement[1]-self.dead_point[1],\n",
    "                     color='grey', length_includes_head=True, width=1.5e-3, head_width=2.0e-2, alpha=0.3)\n",
    "        axs[0].set_xlim(0,1)\n",
    "        axs[0].set_ylim(0,1)\n",
    "        axs[0].set_aspect('equal')\n",
    "        axs[0].set_xlabel(r'$x_0$')\n",
    "        axs[0].set_ylabel(r'$x_1$')\n",
    "        axs[0].legend(loc='upper right', frameon=True)\n",
    "\n",
    "        # \n",
    "        axs[1].set_title('The Likelihood Vs Prior Mass')\n",
    "        axs[1].plot(self.xis, np.exp(self.logLs), color='C0')\n",
    "        if self.iteration<40:\n",
    "            axs[1].scatter(self.xis, np.exp(self.logLs), color='C0', s=20)\n",
    "        axs[1].scatter(self.xis[-1], np.exp(self.log_likelihood(self.dead_point)), color='C1', s=70, \n",
    "                       label='worst point')\n",
    "        for i in range(len(self.xis[1:])):\n",
    "            axs[1].plot([self.xis[i+1],self.xis[i+1]], [0, np.exp(self.logLs[i+1])], color='grey', lw=0.5)\n",
    "        axs[1].set_xlim(0,1)\n",
    "        axs[1].set_ylim(0,1.05*np.exp(self.logLs[-1]))\n",
    "        axs[1].set_xlabel(r'$\\xi$')\n",
    "        axs[1].set_ylabel(r'$L(\\xi)$')\n",
    "        if min(self.xis)>0.25:\n",
    "            axs[1].annotate('', xy=(0,0.45*max(np.exp(self.logLs))), \n",
    "                            xytext=(min(self.xis),0.45*max(np.exp(self.logLs))), \n",
    "                            arrowprops=dict(arrowstyle='<->', color='C2', lw=3))\n",
    "            axs[1].text(0.2*min(self.xis), 0.5*max(np.exp(self.logLs)),\n",
    "                            'replacement point\\n goes in here', color='C2') \n",
    "        axs[1].legend(loc='upper right', frameon=True)\n",
    "        \n",
    "        #\n",
    "        axs[2].set_title('Evidence Estimate')\n",
    "        iter = np.arange(1, 1+self.iteration)\n",
    "        Zest = np.cumsum(np.array(self.weights)*np.exp(self.logLs[1:]))\n",
    "        axs[2].plot(iter,Zest, color='C0')\n",
    "        if self.iteration<40:\n",
    "            axs[2].scatter(iter,Zest, color='C0', s=20)\n",
    "        dZ = 0.05*self.Zexact*np.ones_like(iter)\n",
    "        axs[2].fill_between(iter, Zest-dZ, Zest+dZ, color='C0', alpha=0.2)\n",
    "        axs[2].axhline(self.Zexact, color='k', ls=':', label='analytic value')\n",
    "        axs[2].set_xlabel(r'Iteration $i$')\n",
    "        axs[2].set_ylabel(r'$Z$')\n",
    "        axs[2].legend(loc='lower right')\n",
    "        axs[2].set_xlim(0,self.iteration)\n",
    "        axs[2].set_ylim(0,1.05*max(self.Zexact, np.sum(np.array(self.weights)*np.exp(self.logLs[1:]))))\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "NS = nested_sampling_demo(num_live=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4de5a-4a99-42b2-a628-4180ee95ed69",
   "metadata": {},
   "source": [
    "Run the following cell multiple times to see how the nested sampling algorithm behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5821b5-3ffe-4351-9817-d08804968369",
   "metadata": {},
   "outputs": [],
   "source": [
    "NS.iterate(num_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c952a30b-99f8-49f8-8c7d-212f444a4b88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b9ebf4-3cbd-48ac-b6bf-f6a8e22e24b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8247776-15c4-49a2-a451-8bbf095a59dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
